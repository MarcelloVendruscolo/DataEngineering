{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "greenhouse-facility",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.113:7077\") \\\n",
    "        .appName(\"marcelloVendruscolo_Assignment3_pA\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .config(\"spark.driver.port\",9998)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "#Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "italic-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to (i) lowercase and (ii) tokenize (split on space) text\n",
    "def func_lowercase_split(rdd):\n",
    "    return rdd.lower().split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "naked-receiver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: europarl-v7.sv-en.en\n",
      "Line counting: 1862234\n",
      "Partition counting: 2\n"
     ]
    }
   ],
   "source": [
    "#A.1.1 and A.1.4 - Read the English transcripts with Spark, and count the number of lines and partitions.\n",
    "print(\"File: europarl-v7.sv-en.en\")\n",
    "en_1 = spark_context.textFile(\"hdfs://192.168.2.113:9000/europarl/europarl-v7.sv-en.en\")\n",
    "lineCount_en_1 = en_1.count()\n",
    "print(\"Line counting: \" + str(lineCount_en_1))\n",
    "print(\"Partition counting: \" + str(en_1.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "small-waste",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: europarl-v7.sv-en.sv\n",
      "Line counting: 1862234\n",
      "Partition counting: 3\n"
     ]
    }
   ],
   "source": [
    "#A.1.2 and A.1.4 - Read the Swedish transcripts with Spark, and count the number of lines and partitions.\n",
    "print(\"File: europarl-v7.sv-en.sv\")\n",
    "sv_1 = spark_context.textFile(\"hdfs://192.168.2.113:9000/europarl/europarl-v7.sv-en.sv\")\n",
    "lineCount_sv_1 = sv_1.count()\n",
    "print(\"Line counting: \" + str(lineCount_sv_1))\n",
    "print(\"Partition counting: \" + str(sv_1.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "excessive-escape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line counts are the same for both europarl-v7.sv-en.en and europarl-v7.sv-en.sv?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#A.1.3 - Verify that the line counts are the same for the two languages.\n",
    "print(\"The line counts are the same for both europarl-v7.sv-en.en and europarl-v7.sv-en.sv?\\n\" + str(lineCount_en_1 == lineCount_sv_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "parental-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A.2.1 - Preprocess the text from both RDDs by lowercase-ing and tokenize-ing (split on space) the text:\n",
    "en_2 = en_1.map(func_lowercase_split)\n",
    "sv_2 = sv_1.map(func_lowercase_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "operational-maryland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line counts are the same for europarl-v7.sv-en.en before and after processing?\n",
      "True\n",
      "The line counts are the same for europarl-v7.sv-en.sv before and after processing?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#A.2.3 Verify that the line counts still match after the pre-processing.\n",
    "print(\"The line counts are the same for europarl-v7.sv-en.en before and after processing?\\n\" + str(lineCount_en_1 == en_2.count()))\n",
    "print(\"The line counts are the same for europarl-v7.sv-en.sv before and after processing?\\n\" + str(lineCount_sv_1 == sv_2.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "legendary-library",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 3498375), ('of', 1659758), ('to', 1539760), ('and', 1288401), ('in', 1085993), ('that', 797516), ('a', 773522), ('is', 758050), ('for', 534242), ('we', 522849)]\n",
      "[('att', 1706293), ('och', 1344830), ('i', 1050774), ('det', 924866), ('som', 913276), ('för', 908680), ('av', 738068), ('är', 694381), ('en', 620310), ('vi', 539797)]\n"
     ]
    }
   ],
   "source": [
    "#A.3.1 - Use Spark to compute the 10 most frequently according words in the English and Swedish language corpus.\n",
    "print(en_2.flatMap(lambda x: x).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).takeOrdered(10, key = lambda x: -x[1]))\n",
    "print(sv_2.flatMap(lambda x: x).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).takeOrdered(10, key = lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "combined-matthew",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A.4.1 - Use this parallel corpus to mine some translations in the form of word pairs, for the two languages.\n",
    "en_3 = en_2.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "sv_3 = sv_2.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "en_sv = en_3.join(sv_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "solved-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sv = en_sv.filter(lambda x: (not x[1][0] is None) and (not x[1][1] is None))\n",
    "en_sv = en_sv.filter(lambda x: (len(x[1][0]) != 0) and (len(x[1][1]) != 0))\n",
    "en_sv = en_sv.filter(lambda x: ((len(x[1][0]) <= 15) and (len(x[1][1]) <= 15)))\n",
    "en_sv = en_sv.filter(lambda x: ((len(x[1][0]) >= 2) and (len(x[1][1]) >= 2)))\n",
    "en_sv = en_sv.filter(lambda x: (len(x[1][0]) == len(x[1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "institutional-marsh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tashi', 'tashi'), ('delek!', 'delek!'), ('central', 'parlamentarisk'), ('is', 'kontroll'), ('therefore', 'och'), ('also', 'en'), ('parliamentary', 'medlagstiftandebefogenhet'), ('control', 'är'), ('and', 'också'), ('co-legislation.', 'oumbärligt.'), ('we', 'vi'), ('should', 'måste'), ('draw', 'lära'), ('some', 'oss'), ('lessons', 'läxan'), ('from', 'av'), ('that.', 'det.'), ('thus', 'på'), ('gats', 'det'), ('also', 'sättet')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('is', 'är'), 8820),\n",
       " (('we', 'vi'), 4530),\n",
       " (('i', 'jag'), 3918),\n",
       " (('closed.', 'avslutad.'), 2963),\n",
       " (('this', 'detta'), 2714),\n",
       " (('that', 'det'), 2567),\n",
       " (('it', 'det'), 2351),\n",
       " (('a', 'en'), 2318),\n",
       " (('not', 'inte'), 2186),\n",
       " (('and', 'och'), 2096),\n",
       " (('have', 'har'), 1628),\n",
       " (('\\xa0\\xa0', '\\xa0\\xa0'), 1544),\n",
       " (('are', 'är'), 1530),\n",
       " (('a', 'ett'), 1493),\n",
       " (('this', 'det'), 1430),\n",
       " (('there', 'det'), 1420),\n",
       " (('question', 'fråga'), 1379),\n",
       " (('in', 'i'), 1366),\n",
       " (('the', 'jag'), 1356),\n",
       " (('is', 'debatten'), 1328)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_4 = en_sv.map(lambda x: x[1][0]).flatMap(lambda x: x)\n",
    "sv_4 = en_sv.map(lambda x: x[1][1]).flatMap(lambda x: x)\n",
    "en_sv_2 = en_4.zip(sv_4)\n",
    "print(en_sv_2.take(20))\n",
    "en_sv_2.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).takeOrdered(20, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "becoming-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the cores for another application!\n",
    "#spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-principal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
